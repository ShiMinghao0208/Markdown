# 单应性

## 齐次坐标

在欧几里得几何空间里，两条平行线永远都不会相交。但在投影空间中，两条铁轨在地平线却是相交的，因为在无限远处他们看起来相交于一点。

在欧几里得（或笛卡尔）空间里描述2D/3D物体是很理想的，但在投影空间里面却并不见得，我们用（x，y）表示笛卡尔空间的一个2D点，而处于无限远处的点$(\infin,\infin)$在笛卡尔空间是没有意义的。投影空间的两条平行线会在无限远处相较于一点，但笛卡尔空间是没有办法搞定这个问题（因为无限远处的点在笛卡尔空间是没有意义的），因此数学家想出齐次坐标这个点子来。

**齐次坐标**：能够在投影空间里进行图像和几何处理，齐次坐标用N+1个分量来描述N维坐标。比如2D齐次坐标是在笛卡尔坐标（X,Y）的基础上增加一个新分量w，变成（x，y，w），其中X = x/w，Y = y/w。笛卡尔坐标中的点（1，2）在齐次坐标中就是（1，2，1）。如果这点移动到无限远处，在齐次坐标中就是（1，2，0）,这样我们就避免了用没意义的$\infin$来表示无限远处的点。

概括来说：

- 投影平面上的任何点都可以表示成一三元组 (X, Y, Z)，称之为该点的'齐次坐标或投影坐标，其中 X、Y 及 Z 不全为 0。
- 以齐次坐标表表示的点，若该坐标内的数值全乘上一相同非零实数，仍会表示该点。
- 相反地，两个齐次坐标表示同一点，当且仅当其中一个齐次坐标可由另一个齐次坐标乘上一相同非零常数得取得。
- 当 Z 不为 0，则该点表示欧氏平面上的该 (X/Z, Y/Z)。
- 当 Z 为 0，则该点表示一无穷远点。
- 注意，三元组 (0, 0, 0) 不表示任何点。原点表示为 (0, 0, 1)[3]。

[向量与点的解释](https://blog.csdn.net/winbobob/article/details/38829001)

## 引入齐次坐标的意义

平移变换表示的是位置变化的概念。如下图所示，一个图像矩形从中心点[x1,y1]平移到了中心点[x2,y2]

处，整体大小和角度都没有变化。在x方向和y方向上分别平移了tx和ty大小。

![1](E:\硕士学习\图像处理\单应性\1.png)

显然：

![2](E:\硕士学习\图像处理\单应性\2.png)

这对于图像中的每一个点都是成立的。写成矩阵的形式就是：

![3](E:\硕士学习\图像处理\单应性\3.png)

我们再把前面的缩放变换和旋转变换的矩阵形式写出来：

缩放变换：

![4](E:\硕士学习\图像处理\单应性\4.png)

旋转变换：

![5](E:\硕士学习\图像处理\单应性\5.png)

我们注意到，缩放变换和旋转变换都可以表示成矩阵乘法的形式。实际上，图像的几何变换通常不是单一的，也就是说经常性的缩放、旋转、平移一起变换。例如先放大2倍，然后旋转45度，然后再缩小0.5倍。那么就可以表示成矩阵乘法串接的形式：

![6](E:\硕士学习\图像处理\单应性\6.png)

这样，不管有多少次变换，都可以用矩阵乘法来实现。但是平移变换呢？从前面看到，平移变换并不是矩阵乘法的形式，而是矩阵加法的形式！

那能不能把缩放变换、旋转变换、平移变换统一成矩阵乘法的形式呢，这样不管进行多少次变换，都可以表示成矩阵连乘的形式，将极大的方便计算和降低运算量。

这种方法就是“**升维**”，引入“**齐次坐标**”，将图像从平面2D坐标变成3D坐标。我们看看平移变换的矩阵形式：

![7](E:\硕士学习\图像处理\单应性\7.png)

将其升维，变成3维，上式就可以表示成：

![8](E:\硕士学习\图像处理\单应性\8.png)

这样，平移变换通过升维后的齐次坐标，也变成了矩阵乘法的形式。当然缩放变换和旋转变换的矩阵形式也得改一改，统一变成3维的形式。
缩放变换：

![9](E:\硕士学习\图像处理\单应性\9.png)

旋转变换：

![10](E:\硕士学习\图像处理\单应性\10.png)

以后所有的变换，不管怎样变换，变换多少次，都可以表示成一连串的矩阵相乘了，这就是引入**齐次坐标的作用**，**把各种变换都统一了起来，即 把缩放，旋转，平移等变换都统一起来****，都表示成一连串的矩阵相乘的形式。保证了形式上的线性一致性。**

## 单应性矩阵

两个不同视角的图像上的点对的齐次坐标可以用一个射影变换（projective transformation）表述，即：

x1 =H*x2

![11](E:\硕士学习\图像处理\单应性\11.jpg)



![12](E:\硕士学习\图像处理\单应性\12.jpg)

射影变换也叫“单应”--Homography，“Homo”前缀就是same的意思，表示“同”，homography就是用同一个源产生的graphy，中文译过来大概就是"单应"。因此上面式子中的矩阵H就叫单应性矩阵。上式中的x1和x2都是3*1的齐次坐标，因此H是一个3 * 3的矩阵。如果给定一个单应H，给他的元素乘上同一个数a，得到的单应aH和H作用相同，因为新单应无非把齐次点p1变成了齐次点ap2，ap2和p2对应的图像的点相同，所以一个单应中只有8个自由元素，一般令右下角那个元素$h_{33}=1$来归一化。

8个未知数，需要8个方程来求解，之所以四对点能够求解，是因为一对点提供两个方程。我们假设有两个图像上的点$[x_1,y_1]^T$和$[x_2,y_2]^T$，他们的齐次坐标为：$[x_1,y_1,1]^T$和$[x_2,y_2,1]^T$，所以：

![13](E:\硕士学习\图像处理\单应性\13.png)

[相机标定](https://blog.csdn.net/chentravelling/article/details/53558096)

[单应矩阵]([https://blog.csdn.net/lyhbkz/article/details/82254893?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522159583626519724835843751%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=159583626519724835843751&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v3~pc_rank_v2-7-82254893.first_rank_ecpm_v3_pc_rank_v2&utm_term=%E5%8D%95%E5%BA%94%E6%80%A7%E7%9F%A9%E9%98%B5&spm=1018.2118.3001.4187](https://blog.csdn.net/lyhbkz/article/details/82254893?ops_request_misc=%7B%22request%5Fid%22%3A%22159583626519724835843751%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=159583626519724835843751&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v3~pc_rank_v2-7-82254893.first_rank_ecpm_v3_pc_rank_v2&utm_term=单应性矩阵&spm=1018.2118.3001.4187))

# RANSAC算法

RANSAC是“RANdom SAmple Consensus（随机抽样一致）”的缩写。它可以从一组包含“局外点”的观测数据集中，通过迭代方式估计数学模型的参数。它是一种不确定的算法——它有一定的概率得出一个合理的结果；为了提高概率必须提高迭代次数。

RANSAC的基本假设是：
（1）数据由“局内点”组成，例如：数据的分布可以用一些模型参数来解释；
（2）“局外点”是不能适应该模型的数据；
（3）除此之外的数据属于噪声。
局外点产生的原因有：噪声的极值；错误的测量方法；对数据的错误假设。
RANSAC也做了以下假设：给定一组（通常很小的）局内点，存在一个可以估计模型参数的过程；而该模型能够解释或者适用于局内点。

一、示例
一个简单的例子是从一组观测数据中找出合适的2维直线。假设观测数据中包含局内点和局外点，其中局内点近似的被直线所通过，而局外点远离于直线。简单的最小二乘法不能找到适应于局内点的直线，原因是最小二乘法尽量去适应包括局外点在内的所有点。相反，RANSAC能得出一个仅仅用局内点计算出模型，并且概率还足够高。但是，RANSAC并不能保证结果一定正确，为了保证算法有足够高的合理概率，我们必须小心的选择算法的参数。

二、概述
RANSAC算法的输入是一组观测数据，一个可以解释或者适应于观测数据的参数化模型，一些可信的参数。
RANSAC通过反复选择数据中的一组随机子集来达成目标。被选取的子集被假设为局内点，并用下述方法进行验证：
1.有一个模型适应于假设的局内点，即所有的未知参数都能从假设的局内点计算得出。
2.用1中得到的模型去测试所有的其它数据，如果某个点适用于估计的模型，认为它也是局内点。
3.如果有足够多的点被归类为假设的局内点，那么估计的模型就足够合理。
4.然后，用所有假设的局内点去重新估计模型，因为它仅仅被初始的假设局内点估计过。
5.最后，通过估计局内点与模型的错误率来评估模型。
这个过程被重复执行固定的次数，每次产生的模型要么因为局内点太少而被舍弃，要么因为比现有的模型更好而被选用。

三、算法

伪码：

输入：
data —— 一组观测数据
model —— 适应于数据的模型
n —— 适用于模型的最少数据个数
k —— 算法的迭代次数
t —— 用于决定数据是否适应于模型的阀值
d —— 判定模型是否适用于数据集的数据数目
输出：
best_model —— 跟数据最匹配的模型参数（如果没有找到好的模型，返回null）
best_consensus_set —— 估计出模型的数据点
best_error —— 跟数据相关的估计出的模型错误

iterations = 0
best_model = null
best_consensus_set = null
best_error = 无穷大
while ( iterations < k )
maybe_inliers = 从数据集中随机选择n个点
maybe_model = 适合于maybe_inliers的模型参数
consensus_set = maybe_inliers

for ( 每个数据集中不属于maybe_inliers的点 ）
if ( 如果点适合于maybe_model，且错误小于t ）
将点添加到consensus_set
if （ consensus_set中的元素数目大于d ）
已经找到了好的模型，现在测试该模型到底有多好
better_model = 适合于consensus_set中所有点的模型参数
this_error = better_model究竟如何适合这些点的度量
if ( this_error < best_error )
我们发现了比以前好的模型，保存该模型直到更好的模型出现
best_model = better_model
best_consensus_set = consensus_set
best_error = this_error
增加迭代次数
返回 best_model, best_consensus_set, best_error

RANSAC算法的可能变化包括以下几种：
（1）如果发现了一种足够好的模型（该模型有足够小的错误率），则跳出主循环。这样可能会节约计算额外参数的时间。
（2）直接从maybe_model计算this_error，而不从consensus_set重新估计模型。这样可能会节约比较两种模型错误的时间，但可能会对噪声更敏感。

其实核心就是随机性和假设性。随机性用于减少计算了，那个循环次数就是利用正确数据出现的概率。所谓的假设性，就是说随机抽出来的数据我都认为是正确的，并以此去计算其他点，获得其他满足变换关系的点，然后利用投票机制，选出获票最多的那一个变换。